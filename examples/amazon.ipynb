{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deshift on the WILDS Amazon Reviews Dataset\n",
    "\n",
    "In this notebook, we fine-tune a LLaMA model for sequence classification on the [WILDS Amazon Reviews](https://wilds.stanford.edu/datasets/#amazon) dataset. In this task, the input is a product review written in text, and the label is a number-of-stars from 1 to 5. Importantly, the individuals who write the reviews are known to be disjoint between the train, validation, and test splits of the dataset. As a result, there is a distribution shift.\n",
    "\n",
    "For the LlaMA base model, we use the 58M-parameter [BabyLlaMA](https://huggingface.co/timinar/baby-llama-58m) submission for the BabyLM benchmark. We train two models:\n",
    "- **Empirical Risk Minimization (ERM):** This model is trained by combining all elements in a batch using a flat average.\n",
    "- **Superquantile:** This model is trained using the superquantile/CVaR objective with a head probability of `0.5`. In other words, the top 50% highest losses within each batch are used for backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wilds import get_dataset\n",
    "from wilds.common.data_loaders import get_train_loader\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, Dataset\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from deshift import make_spectral_risk_measure, make_superquantile_spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/mnt/ssd/ronak/datasets/wilds\"\n",
    "CACHE_DIR = \"/mnt/ssd/ronak/models\"\n",
    "SAVE_DIR = \"/mnt/hdd/ronak/wilds/amazon\"\n",
    "OUT_DIR = \"/mnt/ssd/ronak/output/wilds/amazon\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset\n",
    "\n",
    "We tokenize the dataset, which contains about 250,000 reviews in the training set a priori. The goal is to get a sense of the sequence lengths when using the LlaMA tokenizer. Because they are quite long (median `~128`), we subset the dataset to the shorter reviews. The sequence length has a large impact on the eventual memory consumption of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full dataset, and download it if necessary\n",
    "dataset = get_dataset(dataset=\"amazon\", download=True, root_dir=ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"timinar/baby-llama-58m\", cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lens(split, n, tokenizer):\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    lens = []\n",
    "\n",
    "    # For every sentence...\n",
    "    # sentences = dataset[split][\"text\"]\n",
    "    # train_data = dataset.get_subset(split)\n",
    "    loader = get_train_loader(\"standard\", dataset.get_subset(split), batch_size=1)\n",
    "    # if split != \"train\":\n",
    "    #     idx = np.arange(len(sentences))\n",
    "    print(f\"Loader size: {len(loader)}.\")\n",
    "    for i, (x, y, z) in tqdm(enumerate(loader)):\n",
    "        encoded_dict = tokenizer(\n",
    "            x[0], \n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            return_tensors=\"pt\", \n",
    "        )\n",
    "\n",
    "        # Add the encoded sentence to the list.\n",
    "        lens.append(encoded_dict[\"input_ids\"].shape[1])\n",
    "\n",
    "        if i == n - 1:\n",
    "            break\n",
    "\n",
    "    return np.array(lens)\n",
    "\n",
    "\n",
    "# generate encoded tokens:\n",
    "def get_split(split, n, tokenizer, max_length=80):\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "    attn_masks = []\n",
    "    labels = []\n",
    "    metadata = []\n",
    "\n",
    "    # For every sentence...\n",
    "    # sentences = dataset[split][\"text\"]\n",
    "    # train_data = dataset.get_subset(split)\n",
    "    loader = get_train_loader(\"standard\", dataset.get_subset(split), batch_size=1)\n",
    "    # if split != \"train\":\n",
    "    #     idx = np.arange(len(sentences))\n",
    "    print(f\"Loader size: {len(loader)}.\")\n",
    "    for i, (x, y, z) in tqdm(enumerate(loader)):\n",
    "        # first encode fully and check length\n",
    "        encoded_dict = tokenizer(\n",
    "            x[0], \n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            return_tensors=\"pt\", \n",
    "        )\n",
    "\n",
    "        # then continue if length is small\n",
    "        if encoded_dict[\"input_ids\"].shape[1] <= max_length:\n",
    "            encoded_dict = tokenizer(\n",
    "                x[0], \n",
    "                add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors=\"pt\",  # Return pytorch tensors.return_tensors='pt'\n",
    "                return_attention_mask=True,\n",
    "            )\n",
    "\n",
    "            # Add the encoded sentence to the list.\n",
    "            input_ids.append(encoded_dict[\"input_ids\"])\n",
    "            attn_masks.append(encoded_dict[\"attention_mask\"])\n",
    "            labels.append(y.item())\n",
    "            metadata.append(z)\n",
    "\n",
    "        if len(input_ids) == n - 1:\n",
    "            break\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attn_masks = torch.cat(attn_masks, dim=0)\n",
    "    labels = torch.tensor(labels).long()\n",
    "    metadata = torch.cat(metadata)\n",
    "\n",
    "    return input_ids, attn_masks, labels, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "seq_len = get_lens(\"train\", n, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in [0.5, 0.75, 0.9]:\n",
    "    print(f\"{q} quantile: {np.quantile(seq_len, q)}\")\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.hist(seq_len, bins=20, color=\"tab:blue\", alpha=0.5, edgecolor=\"k\")\n",
    "ax.set_xlabel(\"Tokenized Sequence Length\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose `80` as the sequence length and keep only those examples that are shorter than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.inf\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    input_ids, attn_masks, labels, metadata = get_split(split, n, tokenizer, max_length=80)\n",
    "    \n",
    "    print(input_ids.shape)\n",
    "    torch.save(input_ids, os.path.join(SAVE_DIR, f\"{split}_input_ids.pt\"))\n",
    "    torch.save(attn_masks, os.path.join(SAVE_DIR, f\"{split}_attn_masks.pt\"))\n",
    "    torch.save(labels, os.path.join(SAVE_DIR, f\"{split}_labels.pt\"))\n",
    "    torch.save(metadata, os.path.join(SAVE_DIR, f\"{split}_metadata.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "We can begin the training procedure. This involves:\n",
    "- Setting up the superquantile objective, which is only a few extra lines of code.\n",
    "- Setting up the model, which requires a little bit of surgery with the current HuggingFace API. For computational reasons, we only train the last transformer layer (of 16) and the additional \"head\" portion of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 3e-4\n",
    "ADAMW_TOLERANCE = 1e-8\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 3\n",
    "SEED = 123\n",
    "UNFROZEN = 2\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_cost = 0.05\n",
    "penalty = \"chi2\" # options: 'chi2', 'kl'\n",
    "head_prob = 0.5\n",
    "\n",
    "# define spectrum based on the 2-extremile\n",
    "spectrum = make_superquantile_spectrum(BATCH_SIZE, head_prob)\n",
    "\n",
    "# create function which computes weight on each example\n",
    "compute_sample_weight = make_spectral_risk_measure(spectrum, penalty=penalty, shift_cost=shift_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at timinar/baby-llama-58m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(16000, 512, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "          (up_proj): Linear(in_features=512, out_features=1024, bias=False)\n",
       "          (down_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (score): Linear(in_features=512, out_features=5, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## model\n",
    "n_labels = 5\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"timinar/baby-llama-58m\", cache_dir=CACHE_DIR)\n",
    "\n",
    "# model surgery\n",
    "model.score = nn.Linear(in_features=512, out_features=n_labels, bias=False)\n",
    "model.num_labels = n_labels\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for module in model.model.layers[-UNFROZEN:] + [model.model.norm, model.score]:\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Amazon(Dataset):\n",
    "    def __init__(self, input_ids, attn_masks, labels, metadata):\n",
    "        self.input_ids = input_ids\n",
    "        self.attn_masks = attn_masks\n",
    "        self.labels = labels\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.input_ids[i], self.attn_masks[i], self.labels[i], self.metadata[i]\n",
    "    \n",
    "def load_dataset(split):\n",
    "    input_ids = torch.load(os.path.join(SAVE_DIR, f\"{split}_input_ids.pt\"))\n",
    "    attn_masks = torch.load(os.path.join(SAVE_DIR, f\"{split}_attn_masks.pt\"))\n",
    "    labels = torch.load(os.path.join(SAVE_DIR, f\"{split}_labels.pt\"))\n",
    "    metadata = torch.load(os.path.join(SAVE_DIR, f\"{split}_metadata.pt\"))\n",
    "\n",
    "    return Amazon(input_ids, attn_masks, labels, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 0.05G\n",
      "85,586 training samples.\n",
      "38,669 validation samples.\n"
     ]
    }
   ],
   "source": [
    "# small enough to fit in CPU memory\n",
    "train_dataset = load_dataset(\"train\")\n",
    "val_dataset = load_dataset(\"val\")\n",
    "\n",
    "a = train_dataset.input_ids\n",
    "print(f\"Train dataset size: {a.element_size() * a.numel() / (1024 ** 3):0.2f}G\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, sampler=RandomSampler(train_dataset), batch_size=BATCH_SIZE\n",
    ")\n",
    "print(\"{:>5,} training samples.\".format(len(train_dataset)))\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset, sampler=SequentialSampler(val_dataset), batch_size=BATCH_SIZE\n",
    ")\n",
    "print(\"{:>5,} validation samples.\".format(len(val_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also note that the classes are quite imbalanced in this dataset. We make the conscious choice to keep them this way and see if the risk-sensitive model can control error in the less frequent classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01191784 0.02264389 0.08014161 0.26771902 0.61757764]\n",
      "[0.01399054 0.02280897 0.07626264 0.24254571 0.64439215]\n"
     ]
    }
   ],
   "source": [
    "print(np.bincount(train_dataset.labels.numpy()) / len(train_dataset))\n",
    "print(np.bincount(val_dataset.labels.numpy()) / len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = LEARNING_RATE, eps = ADAMW_TOLERANCE)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = EPOCHS * BATCH_SIZE * len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dict_of_lists(lst):\n",
    "    return {key: [i[key] for i in lst] for key in lst[0]}\n",
    "\n",
    "def format_time(elapsed):\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "  Batch   100  of    669.    Elapsed: 0:00:16.   Loss: 1.26852\n",
      "  Batch   200  of    669.    Elapsed: 0:00:31.   Loss: 1.21709\n",
      "  Batch   300  of    669.    Elapsed: 0:00:47.   Loss: 1.18519\n",
      "  Batch   400  of    669.    Elapsed: 0:01:03.   Loss: 1.16590\n",
      "  Batch   500  of    669.    Elapsed: 0:01:19.   Loss: 1.14959\n",
      "  Batch   600  of    669.    Elapsed: 0:01:35.   Loss: 1.13724\n",
      "\n",
      "  Average training loss: 1.13\n",
      "  Average training accuracy: 0.58\n",
      "  Training epoch took: 0:01:46\n",
      "\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.32\n",
      "  Validation Loss: 0.92\n",
      "  Validation took: 0:00:38\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "  Batch   100  of    669.    Elapsed: 0:00:16.   Loss: 1.02922\n",
      "  Batch   200  of    669.    Elapsed: 0:00:32.   Loss: 1.02772\n",
      "  Batch   300  of    669.    Elapsed: 0:00:48.   Loss: 1.02707\n",
      "  Batch   400  of    669.    Elapsed: 0:01:03.   Loss: 1.02835\n",
      "  Batch   500  of    669.    Elapsed: 0:01:19.   Loss: 1.02296\n",
      "  Batch   600  of    669.    Elapsed: 0:01:35.   Loss: 1.02173\n",
      "\n",
      "  Average training loss: 1.02\n",
      "  Average training accuracy: 0.62\n",
      "  Training epoch took: 0:01:46\n",
      "\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.42\n",
      "  Validation Loss: 0.89\n",
      "  Validation took: 0:00:38\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "  Batch   100  of    669.    Elapsed: 0:00:16.   Loss: 0.95638\n",
      "  Batch   200  of    669.    Elapsed: 0:00:32.   Loss: 0.95872\n",
      "  Batch   300  of    669.    Elapsed: 0:00:47.   Loss: 0.96534\n",
      "  Batch   400  of    669.    Elapsed: 0:01:03.   Loss: 0.96871\n",
      "  Batch   500  of    669.    Elapsed: 0:01:18.   Loss: 0.96969\n",
      "  Batch   600  of    669.    Elapsed: 0:01:34.   Loss: 0.96813\n",
      "\n",
      "  Average training loss: 0.97\n",
      "  Average training accuracy: 0.63\n",
      "  Training epoch took: 0:01:45\n",
      "\n",
      "Running Validation...\n",
      "  Validation Accuracy: 0.70\n",
      "  Validation Loss: 0.86\n",
      "  Validation took: 0:00:37\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:07:10 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "# Seed everything.\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "for epoch_i in range(EPOCHS):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"======== Epoch {:} / {:} ========\".format(epoch_i + 1, EPOCHS))\n",
    "    print(\"Training...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    total_train_accuracy = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print(\n",
    "                \"  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.   Loss: {:0.5f}\".format(\n",
    "                    step, len(train_dataloader), elapsed, total_train_loss / step\n",
    "                )\n",
    "            )\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        output = model(\n",
    "            input_ids=b_input_ids,\n",
    "            attention_mask=b_input_mask,\n",
    "            labels=b_labels,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        # loss = output.loss\n",
    "        logits = output.logits\n",
    "\n",
    "        losses = F.cross_entropy(logits, b_labels, reduction=\"none\")\n",
    "\n",
    "        # one line of code addition!\n",
    "        weights = compute_sample_weight(losses)\n",
    "\n",
    "        loss = weights @ losses\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "\n",
    "        # TODO: See if this is needed.\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_train_accuracy += flat_accuracy(logits.detach().cpu().numpy(), b_labels.detach().cpu().numpy())\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Average training accuracy: {0:.2f}\".format(avg_train_accuracy))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(\n",
    "                input_ids=b_input_ids,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels,\n",
    "            )\n",
    "            loss = output.loss\n",
    "            logits = output.logits\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to(\"cpu\").numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Validation Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            \"epoch\": epoch_i + 1,\n",
    "            \"Training Loss\": avg_train_loss,\n",
    "            \"Training Accur.\": avg_train_accuracy,\n",
    "            \"Valid. Loss\": avg_val_loss,\n",
    "            \"Valid. Accur.\": avg_val_accuracy,\n",
    "            \"Training Time\": training_time,\n",
    "            \"Validation Time\": validation_time,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\n",
    "    \"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0))\n",
    ")\n",
    "\n",
    "# Save the model.\n",
    "model_name = \"superquantile\" if head_prob > 0.0 else \"erm\"\n",
    "torch.save(model, os.path.join(OUT_DIR, f\"{model_name}.pt\"))\n",
    "training_stats = to_dict_of_lists(training_stats)\n",
    "with open(os.path.join(OUT_DIR, f\"{model_name}.json\"), \"w\") as f:\n",
    "    json.dump(training_stats, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Performance\n",
    "\n",
    "We inspect the test performance of the two models, which were trained with identical parameters, except one used the `0.5`-superquantile. We can compare models across a few axes:\n",
    "- **Classes**: Given the class imbalance, we can look at per class precision and recall.\n",
    "- **Metadata Groups**: Subpopulations are given in the dataset from the groupings.\n",
    "As an initial pass, we also look at accuracy as the test set is drawn from a different distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "erm = torch.load(os.path.join(OUT_DIR, f\"erm.pt\"))\n",
    "erm.eval()\n",
    "\n",
    "superquantile = torch.load(os.path.join(OUT_DIR, f\"superquantile.pt\"))\n",
    "superquantile.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small enough to fit in CPU memory\n",
    "test_dataset = load_dataset(\"test\")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, sampler=RandomSampler(test_dataset), batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(model):\n",
    "    logit_list = []\n",
    "    label_list = []\n",
    "    meta_list = []\n",
    "    for batch in tqdm(test_dataloader):\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        b_metadata = batch[3]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(\n",
    "                input_ids=b_input_ids,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels,\n",
    "            )\n",
    "            loss = output.loss\n",
    "            logits = output.logits\n",
    "\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to(\"cpu\").numpy()\n",
    "\n",
    "        logit_list.append(logits)\n",
    "        label_list.append(label_ids)\n",
    "        meta_list.append(b_metadata.numpy())\n",
    "\n",
    "    scores = np.concatenate(logit_list, axis=0)\n",
    "    y_true = np.concatenate(label_list)\n",
    "    metadata = np.concatenate(meta_list, axis=0)\n",
    "\n",
    "    return y_true, scores, metadata\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ERM accuracy is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306/306 [00:36<00:00,  8.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.661597832754038"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_erm, scores_erm, meta_list_erm = get_scores(erm)\n",
    "flat_accuracy(scores_erm, y_true_erm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The superquantile accuracy is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306/306 [00:38<00:00,  7.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6771876916785934"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_sq, scores_sq, meta_list_sq = get_scores(superquantile)\n",
    "flat_accuracy(scores_sq, y_true_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note though, that this is not far from the chance level, which is about `0.61`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0155643 , 0.02655387, 0.08344408, 0.25562257, 0.61881517])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_true_sq) / len(y_true_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at the full classification report to compare the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.32      0.33       609\n",
      "           1       0.32      0.27      0.30      1039\n",
      "           2       0.42      0.43      0.42      3265\n",
      "           3       0.46      0.41      0.44     10002\n",
      "           4       0.78      0.82      0.80     24213\n",
      "\n",
      "    accuracy                           0.66     39128\n",
      "   macro avg       0.46      0.45      0.46     39128\n",
      "weighted avg       0.65      0.66      0.66     39128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true_erm, np.argmax(scores_erm, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.15      0.22       609\n",
      "           1       0.34      0.39      0.36      1039\n",
      "           2       0.43      0.41      0.42      3265\n",
      "           3       0.52      0.23      0.32     10002\n",
      "           4       0.74      0.92      0.82     24213\n",
      "\n",
      "    accuracy                           0.68     39128\n",
      "   macro avg       0.49      0.42      0.43     39128\n",
      "weighted avg       0.64      0.68      0.64     39128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true_sq, np.argmax(scores_sq, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are notable increases in the precision of the superquantile model across the uncommon classes, which account for less than 10% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
