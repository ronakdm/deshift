{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deshift on the WILDS Amazon Reviews Dataset\n",
    "\n",
    "In this notebook, we fine-tune a LLaMA model for sequence classification on the [WILDS Amazon Reviews](https://wilds.stanford.edu/datasets/#amazon) dataset. In this task, the input is a product review written in text, and the label is a number-of-stars from 1 to 5. Importantly, the individuals who write the reviews are known to be disjoint between the train, validation, and test splits of the dataset. As a result, there is a distribution shift.\n",
    "\n",
    "For the LlaMA base model, we use the 58M-parameter [BabyLlaMA](https://huggingface.co/timinar/baby-llama-58m) submission for the BabyLM benchmark. We train two models:\n",
    "- **Empirical Risk Minimization (ERM):** This model is trained by combining all elements in a batch using a flat average.\n",
    "- **Superquantile:** This model is trained using the superquantile/CVaR objective with a head probability of `0.5`. In other words, the top 50% highest losses within each batch are used for backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wilds import get_dataset\n",
    "from wilds.common.data_loaders import get_train_loader\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, get_cosine_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, Dataset\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from deshift import make_spectral_risk_measure, make_superquantile_spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/mnt/ssd/ronak/datasets/wilds\"\n",
    "CACHE_DIR = \"/mnt/ssd/ronak/models\"\n",
    "SAVE_DIR = \"/mnt/hdd/ronak/wilds/amazon\"\n",
    "OUT_DIR = \"/mnt/ssd/ronak/output/wilds/amazon\"\n",
    "MODEL_NAME = \"llama\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset\n",
    "\n",
    "We tokenize the dataset, which contains about 250,000 reviews in the training set a priori. The goal is to get a sense of the sequence lengths when using the LlaMA tokenizer. Because they are quite long (median `~128`), we subset the dataset to the shorter reviews. The sequence length has a large impact on the eventual memory consumption of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = 5000\n",
    "SEQ_LEN = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"timinar/baby-llama-58m\", cache_dir=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full dataset, and download it if necessary\n",
    "dataset = get_dataset(dataset=\"amazon\", download=True, root_dir=ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lens(split, n, tokenizer):\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    lens = []\n",
    "\n",
    "    loader = get_train_loader(\"standard\", dataset.get_subset(split), batch_size=1)\n",
    "    print(f\"Loader size: {len(loader)}.\")\n",
    "    for i, (x, y, z) in tqdm(enumerate(loader)):\n",
    "        encoded_dict = tokenizer(\n",
    "            x[0], \n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            return_tensors=\"pt\", \n",
    "        )\n",
    "\n",
    "        # Add the encoded sentence to the list.\n",
    "        lens.append(encoded_dict[\"input_ids\"].shape[1])\n",
    "\n",
    "        if i == n - 1:\n",
    "            break\n",
    "\n",
    "    return np.array(lens)\n",
    "\n",
    "\n",
    "# generate encoded tokens:\n",
    "def get_split(split, tokenizer, max_length=80, idx=None):\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "    attn_masks = []\n",
    "    labels = []\n",
    "    metadata = []\n",
    "\n",
    "    # For every sentence...\n",
    "    # sentences = dataset[split][\"text\"]\n",
    "    # train_data = dataset.get_subset(split)\n",
    "    loader = get_train_loader(\"standard\", dataset.get_subset(split), batch_size=1)\n",
    "    print(f\"Loader size: {len(loader)}.\")\n",
    "    for i, (x, y, z) in tqdm(enumerate(loader)):\n",
    "        if idx is None or i in idx:\n",
    "            encoded_dict = tokenizer(\n",
    "                x[0], \n",
    "                add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "                max_length=max_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors=\"pt\",  # Return pytorch tensors.return_tensors='pt'\n",
    "                return_attention_mask=True,\n",
    "            )\n",
    "\n",
    "            # Add the encoded sentence to the list.\n",
    "            input_ids.append(encoded_dict[\"input_ids\"])\n",
    "            attn_masks.append(encoded_dict[\"attention_mask\"])\n",
    "            labels.append(y.item())\n",
    "            metadata.append(z)\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attn_masks = torch.cat(attn_masks, dim=0)\n",
    "    labels = torch.tensor(labels).long()\n",
    "    metadata = torch.cat(metadata)\n",
    "\n",
    "    return input_ids, attn_masks, labels, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader size: 245502.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (136 > 128). Running this sequence through the model will result in indexing errors\n",
      "2999it [00:01, 1508.22it/s]\n"
     ]
    }
   ],
   "source": [
    "n = 3000\n",
    "seq_len = get_lens(\"train\", n, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 quantile: 124.0\n",
      "0.75 quantile: 245.0\n",
      "0.9 quantile: 383.0999999999999\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for q in [0.5, 0.75, 0.9]:\n",
    "    print(f\"{q} quantile: {np.quantile(seq_len, q)}\")\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.hist(seq_len, bins=20, color=\"tab:blue\", alpha=0.5, edgecolor=\"k\")\n",
    "ax.set_xlabel(\"Tokenized Sequence Length\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loader size: 245502.\n",
      "val loader size: 100050.\n",
      "test loader size: 100050.\n"
     ]
    }
   ],
   "source": [
    "# loader sizes\n",
    "sample_sizes = {}\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    loader = get_train_loader(\"standard\", dataset.get_subset(split), batch_size=1)\n",
    "    print(f\"{split} loader size: {len(loader)}.\")\n",
    "    sample_sizes[split] = len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader size: 245502.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "245502it [00:22, 10728.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 80])\n",
      "Loader size: 100050.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100050it [00:13, 7239.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 80])\n",
      "Loader size: 100050.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100050it [00:13, 7511.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 80])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    idx = np.random.choice(sample_sizes[split], DATA_SIZE, replace=False)\n",
    "    input_ids, attn_masks, labels, metadata = get_split(split, tokenizer, max_length=80, idx=idx)\n",
    "    \n",
    "    print(input_ids.shape)\n",
    "    torch.save(input_ids, os.path.join(SAVE_DIR, f\"{MODEL_NAME}_{split}_input_ids.pt\"))\n",
    "    torch.save(attn_masks, os.path.join(SAVE_DIR, f\"{MODEL_NAME}_{split}_attn_masks.pt\"))\n",
    "    torch.save(labels, os.path.join(SAVE_DIR, f\"{MODEL_NAME}_{split}_labels.pt\"))\n",
    "    torch.save(metadata, os.path.join(SAVE_DIR, f\"{MODEL_NAME}_{split}_metadata.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "We can begin the training procedure. This involves:\n",
    "- Setting up the superquantile objective, which is only a few extra lines of code.\n",
    "- Setting up the model, which requires a little bit of surgery with the current HuggingFace API. For computational reasons, we only train the last transformer layer (of 16) and the additional \"head\" portion of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 6e-5\n",
    "ADAMW_TOLERANCE = 1e-8\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "SEED = 123\n",
    "OBJECTIVE = \"erm\"\n",
    "DEVICE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(split):\n",
    "    input_ids  = torch.load(os.path.join(SAVE_DIR, f\"{MODEL_NAME}_{split}_input_ids.pt\"))\n",
    "    attn_masks = torch.load(os.path.join(SAVE_DIR, f\"{MODEL_NAME}_{split}_attn_masks.pt\"))\n",
    "    labels     = torch.load(os.path.join(SAVE_DIR, f\"{MODEL_NAME}_{split}_labels.pt\"))\n",
    "    metadata   = torch.load(os.path.join(SAVE_DIR, f\"{MODEL_NAME}_{split}_metadata.pt\"))\n",
    "    return input_ids, attn_masks, labels, metadata\n",
    "\n",
    "class MaskedSequenceClassificationDataset(Dataset):\n",
    "    def __init__(self, input_ids, attn_masks, labels, metadata):\n",
    "        self.input_ids = input_ids\n",
    "        self.attn_masks = attn_masks\n",
    "        self.labels = labels\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.input_ids[i], self.attn_masks[i], self.labels[i], self.metadata[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5,000 training samples.\n",
      "5,000 validation samples.\n",
      "5,000 test samples.\n"
     ]
    }
   ],
   "source": [
    "input_ids, attn_masks, labels, metadata = load_data(\"train\")\n",
    "train_dataset = MaskedSequenceClassificationDataset(input_ids, attn_masks, labels, metadata)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, sampler=RandomSampler(train_dataset), batch_size=BATCH_SIZE\n",
    ")\n",
    "print(\"{:>5,} training samples.\".format(len(train_dataset)))\n",
    "\n",
    "input_ids, attn_masks, labels, metadata = load_data(\"val\")\n",
    "val_dataset = MaskedSequenceClassificationDataset(input_ids, attn_masks, labels, metadata)\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset, sampler=SequentialSampler(val_dataset), batch_size=BATCH_SIZE\n",
    ")\n",
    "print(\"{:>5,} validation samples.\".format(len(val_dataset)))\n",
    "\n",
    "input_ids, attn_masks, labels, metadata = load_data(\"test\")\n",
    "test_dataset = MaskedSequenceClassificationDataset(input_ids, attn_masks, labels, metadata)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, sampler=RandomSampler(test_dataset), batch_size=BATCH_SIZE\n",
    ")\n",
    "print(\"{:>5,} test samples.\".format(len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0096 0.026  0.0948 0.3036 0.566 ]\n",
      "[0.0126 0.0302 0.0924 0.276  0.5888]\n"
     ]
    }
   ],
   "source": [
    "print(np.bincount(train_dataset.labels.numpy()) / len(train_dataset))\n",
    "print(np.bincount(val_dataset.labels.numpy()) / len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"timinar/baby-llama-58m\",\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    cache_dir=CACHE_DIR,\n",
    ").to(DEVICE)\n",
    "\n",
    "# n_labels = 5\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"timinar/baby-llama-58m\", cache_dir=CACHE_DIR)\n",
    "# model surgery\n",
    "# model.score = nn.Linear(in_features=512, out_features=n_labels, bias=False)\n",
    "# model.num_labels = n_labels\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for module in model.model.layers[-UNFROZEN:] + [model.model.norm, model.score]:\n",
    "#     for param in module.parameters():\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also note that the classes are quite imbalanced in this dataset. We make the conscious choice to keep them this way and see if the risk-sensitive model can control error in the less frequent classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0096 0.026  0.0948 0.3036 0.566 ]\n",
      "[0.0126 0.0302 0.0924 0.276  0.5888]\n"
     ]
    }
   ],
   "source": [
    "print(np.bincount(train_dataset.labels.numpy()) / len(train_dataset))\n",
    "print(np.bincount(val_dataset.labels.numpy()) / len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = LEARNING_RATE, eps = ADAMW_TOLERANCE)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = EPOCHS * BATCH_SIZE * len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dict_of_lists(lst):\n",
    "    return {key: [i[key] for i in lst] for key in lst[0]}\n",
    "\n",
    "def format_time(elapsed):\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of    157.    Elapsed: 0:00:05.   Loss: 3.39548\n",
      "  Batch    80  of    157.    Elapsed: 0:00:10.   Loss: 3.36823\n",
      "  Batch   120  of    157.    Elapsed: 0:00:14.   Loss: 3.36275\n",
      "\n",
      "  Average training loss: 3.346\n",
      "  Training epoch took: 0:00:19\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 3.462\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of    157.    Elapsed: 0:00:05.   Loss: 3.08475\n",
      "  Batch    80  of    157.    Elapsed: 0:00:10.   Loss: 3.10151\n",
      "  Batch   120  of    157.    Elapsed: 0:00:15.   Loss: 3.10533\n",
      "\n",
      "  Average training loss: 3.108\n",
      "  Training epoch took: 0:00:19\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 3.456\n",
      "  Validation took: 0:00:06\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:00:50 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "# Seed everything.\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "for epoch_i in range(EPOCHS):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"======== Epoch {:} / {:} ========\".format(epoch_i + 1, EPOCHS))\n",
    "    print(\"Training...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    total_train_accuracy = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print(\n",
    "                \"  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.   Loss: {:0.5f}\".format(\n",
    "                    step, len(train_dataloader), elapsed, total_train_loss / step\n",
    "                )\n",
    "            )\n",
    "\n",
    "        b_input_ids = batch[0].to(DEVICE)\n",
    "        b_input_mask = batch[1].to(DEVICE)\n",
    "        b_labels = batch[2].to(DEVICE)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        output = model(\n",
    "            input_ids=b_input_ids,\n",
    "            attention_mask=b_input_mask,\n",
    "            labels=b_input_ids,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        loss = output.loss\n",
    "        # logits = output.logits\n",
    "\n",
    "        # one line of code addition!\n",
    "        # losses = F.cross_entropy(logits, b_labels, reduction=\"none\")\n",
    "        # weights = compute_sample_weight(losses)\n",
    "        # loss = weights @ losses\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "\n",
    "        # TODO: See if this is needed.\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # total_train_accuracy += flat_accuracy(logits.detach().cpu().numpy(), b_labels.detach().cpu().numpy())\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.3f}\".format(avg_train_loss))\n",
    "    # print(\"  Average training accuracy: {0:.3f}\".format(avg_train_accuracy))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "\n",
    "        b_input_ids = batch[0].to(DEVICE)\n",
    "        b_input_mask = batch[1].to(DEVICE)\n",
    "        b_labels = batch[2].to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(\n",
    "                input_ids=b_input_ids,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_input_ids,\n",
    "            )\n",
    "            loss = output.loss\n",
    "            logits = output.logits\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # logits = logits.detach().cpu().numpy()\n",
    "        # label_ids = b_labels.to(\"cpu\").numpy()\n",
    "\n",
    "        # total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    # avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    # print(\"  Validation Accuracy: {0:.3f}\".format(avg_val_accuracy))\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            \"epoch\": epoch_i + 1,\n",
    "            \"Training Loss\": avg_train_loss,\n",
    "            # \"Training Accur.\": avg_train_accuracy,\n",
    "            \"Valid. Loss\": avg_val_loss,\n",
    "            # \"Valid. Accur.\": avg_val_accuracy,\n",
    "            \"Training Time\": training_time,\n",
    "            \"Validation Time\": validation_time,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\n",
    "    \"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0))\n",
    ")\n",
    "\n",
    "# Save the model.\n",
    "torch.save(model.state_dict(), os.path.join(OUT_DIR, f\"{MODEL_NAME}_{OBJECTIVE}.pt\"))\n",
    "training_stats = to_dict_of_lists(training_stats)\n",
    "with open(os.path.join(OUT_DIR, f\"{MODEL_NAME}_{OBJECTIVE}.json\"), \"w\") as f:\n",
    "    json.dump(training_stats, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Performance\n",
    "\n",
    "We inspect the test performance of the two models, which were trained with identical parameters, except one used the `0.5`-superquantile. We can compare models across a few axes:\n",
    "- **Classes**: Given the class imbalance, we can look at per class precision and recall.\n",
    "- **Metadata Groups**: Subpopulations are given in the dataset from the groupings.\n",
    "As an initial pass, we also look at accuracy as the test set is drawn from a different distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "erm = torch.load(os.path.join(OUT_DIR, f\"erm.pt\"))\n",
    "erm.eval()\n",
    "\n",
    "superquantile = torch.load(os.path.join(OUT_DIR, f\"superquantile.pt\"))\n",
    "superquantile.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small enough to fit in CPU memory\n",
    "test_dataset = load_dataset(\"test\")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, sampler=RandomSampler(test_dataset), batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(model):\n",
    "    logit_list = []\n",
    "    label_list = []\n",
    "    meta_list = []\n",
    "    for batch in tqdm(test_dataloader):\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        b_metadata = batch[3]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(\n",
    "                input_ids=b_input_ids,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels,\n",
    "            )\n",
    "            loss = output.loss\n",
    "            logits = output.logits\n",
    "\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to(\"cpu\").numpy()\n",
    "\n",
    "        logit_list.append(logits)\n",
    "        label_list.append(label_ids)\n",
    "        meta_list.append(b_metadata.numpy())\n",
    "\n",
    "    scores = np.concatenate(logit_list, axis=0)\n",
    "    y_true = np.concatenate(label_list)\n",
    "    metadata = np.concatenate(meta_list, axis=0)\n",
    "\n",
    "    return y_true, scores, metadata\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ERM accuracy is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306/306 [00:36<00:00,  8.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.661597832754038"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_erm, scores_erm, meta_list_erm = get_scores(erm)\n",
    "flat_accuracy(scores_erm, y_true_erm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The superquantile accuracy is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306/306 [00:38<00:00,  7.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6771876916785934"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_sq, scores_sq, meta_list_sq = get_scores(superquantile)\n",
    "flat_accuracy(scores_sq, y_true_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note though, that this is not far from the chance level, which is about `0.61`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0155643 , 0.02655387, 0.08344408, 0.25562257, 0.61881517])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_true_sq) / len(y_true_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at the full classification report to compare the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.32      0.33       609\n",
      "           1       0.32      0.27      0.30      1039\n",
      "           2       0.42      0.43      0.42      3265\n",
      "           3       0.46      0.41      0.44     10002\n",
      "           4       0.78      0.82      0.80     24213\n",
      "\n",
      "    accuracy                           0.66     39128\n",
      "   macro avg       0.46      0.45      0.46     39128\n",
      "weighted avg       0.65      0.66      0.66     39128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true_erm, np.argmax(scores_erm, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.15      0.22       609\n",
      "           1       0.34      0.39      0.36      1039\n",
      "           2       0.43      0.41      0.42      3265\n",
      "           3       0.52      0.23      0.32     10002\n",
      "           4       0.74      0.92      0.82     24213\n",
      "\n",
      "    accuracy                           0.68     39128\n",
      "   macro avg       0.49      0.42      0.43     39128\n",
      "weighted avg       0.64      0.68      0.64     39128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true_sq, np.argmax(scores_sq, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are notable increases in the precision of the superquantile model across the uncommon classes, which account for less than 10% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
